{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Decision trees\n",
    "\n",
    "#### Question 1. Which of these problems does not fall into 3 main types of ML tasks: classification, regression, and clustering?\n",
    "\n",
    "- Identifying a topic of a live-chat with a customer.  \n",
    "Определение темы живого чата с клиентом. Классификация \n",
    "\n",
    "\n",
    "- Grouping news into topics.  \n",
    "Группировка новостей в темы. Кластеризация \n",
    "\n",
    "\n",
    "- **Predicting LTV (Life-Time Value) - the amount of money spent by a customer in a certain large period of time.**\n",
    " \n",
    "\n",
    "\n",
    "- Listing top products that a user is prone to buy (based on his/her click history).  \n",
    "Кластеризация\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. Maximal possible entropy is achieved when all states are equally probable (prove it yourself for a system with 2 states with probabilities p and 1−p). What's the maximal possible entropy of a system with N states? (here all logs are with base 2)\n",
    "\n",
    "- $ N\\log N $\n",
    "\n",
    "\n",
    "- $ −\\log N $\n",
    "\n",
    "\n",
    "-  $\\log N$ **+**\n",
    "\n",
    "\n",
    "- −$N \\log N$\n",
    "\n",
    "___\n",
    "\n",
    "$$ \\Large S = -\\sum_{i=1}^{N}p_i \\log_2{p_i} $$\n",
    "\n",
    "Максимальная энтропия, при условии N состояний - N разных объёктов. Т.о. вероятность встретить объект $\\large p_i = p = \\frac{1}{N}$. Получаем:\n",
    "$$ S = -\\sum_{i=1}^{N}p_i \\log_2{p_i} = -\\sum_{i=1}^{N} \\frac{1}{N} \\log_2{\\frac{1}{N}} = - \\frac{N}{N} \\log_2{\\frac{1}{N}} =  \\log_2{N}$$\n",
    "\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3. In Topic 3 article, toy example with 20 balls, what's the information gain of splitting 20 balls in 2 groups based on the condition X <= 8?\n",
    "\n",
    "- ~ 0.1\n",
    "\n",
    "\n",
    "- ~ 0.01\n",
    "\n",
    "\n",
    "- ~ 0.001\n",
    "\n",
    "\n",
    "- ~ **0.0001 +**\n",
    "\n",
    "<img src=\"https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/img/topic3_entropy_balls1.png\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S_0 = -\\frac{9}{20}\\log_2{\\frac{9}{20}}-\\frac{11}{20}\\log_2{\\frac{11}{20}} \\approx 1 $$\n",
    "\n",
    "$$ \\Large IG(Q) = S_O - \\sum_{i=1}^{q}\\frac{N_i}{N}S_i, $$\n",
    "$$  S_i = -\\sum_{i=1}^{N_i}p_i \\log_2{p_i} $$\n",
    "\n",
    "**X <= 8:**\n",
    "- Первая группа: 4 синих и 5 оранжевых\n",
    "\n",
    "\n",
    "- Вторая группа: 5 синих и 6 оранжевых"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927744539878083"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "S_0 = - ( 9/20 * np.log2(9/20)  + 11/20 * np.log2(11/20) )\n",
    "S_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910760598382222\n",
      "0.9940302114769565\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "S_l = - ( 4/9 * np.log2(4/9) + 5/9 * np.log2(5/9) )\n",
    "print(S_l)\n",
    "\n",
    "S_r = - ( 5/11 * np.log2(5/11) + 6/11 * np.log2(6/11) )\n",
    "print(S_r)\n",
    "\n",
    "IG = S_0 - 9/20 * S_l  - 11/20 * S_r\n",
    "print(round(IG, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4. In a toy binary classification task, there are d features $x_1 \\ldots x_d$, but target y depends only on $x_1$ and $x_2$: $y = [\\frac{x_1^2}{4} + \\frac{x_2^2}{9} \\leq 16]$, where [⋅] is an indicator function. All of features x3…xd are noisy, i.e. do not influence the target feature at all. Obviously, machine learning algorithms shall perform almost perfectly in this task, where target is a simple function of input features. \n",
    "\n",
    "If we train sklearn's DecisionTreeClassifier for this task, which parameters have crucial effect on accuracy (crucial - meaning that if these parameters are set incorrectly, then accuracy can drop significantly)? Select all that apply (to get credits, you need to select all that apply, no partially correct answers).\n",
    "\n",
    "\n",
    "- max_features\n",
    "\n",
    "\n",
    "- criterion\n",
    "\n",
    "\n",
    "- **min_samples_leaf +**\n",
    "\n",
    "\n",
    "- **max_depth +**\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5. \n",
    "- Load iris data with sklearn.datasets.load_iris. \n",
    "\n",
    "\n",
    "- Train a decision tree with this data, specifying params max_depth=4 and random_state=17 (all other arguments shall be left unchanged).\n",
    "\n",
    "\n",
    "- Use all available 150 instances to train a tree (do not perform train/validation split). \n",
    "\n",
    "\n",
    "- Visualize the fitted decision tree, see topic 3 for examples. \n",
    "\n",
    "\n",
    "- Let's call a leaf in a tree pure if it contains instances of only one class. How many pure leaves are there in this tree?\n",
    "\n",
    "\n",
    "    - 6\n",
    "    - 7 +\n",
    "    - 8\n",
    "    - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = load_iris()['feature_names']\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4,\n",
    "                              random_state=17).fit(load_iris().data,\n",
    "                                                   load_iris().target)\n",
    "export_graphviz(tree,\n",
    "                feature_names=feature_names,\n",
    "                out_file=\"some.dot\", filled=True\n",
    "               )\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file(\"some.dot\")\n",
    "graph.write_png('somefile.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"somefile.png\">\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Ensembles and Random Forest\n",
    "\n",
    "#### Question 6. \n",
    "- There are 7 jurors in the courtroom. Each of them individually can correctly determine whether the defendant is guilty or not with 80% probability. \n",
    "\n",
    "How likely is the jury will make a correct verdict jointly if the decision is made by majority voting?\n",
    "\n",
    "- 20.97%\n",
    "- 80.00%\n",
    "- 83.70%\n",
    "- **96.66% +**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666560000000004\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import binom\n",
    "\n",
    "n = 7\n",
    "negative = n // 2\n",
    "\n",
    "p_neg = 0.2\n",
    "p_pos = 0.8\n",
    "\n",
    "prob = 0\n",
    "for k in range(negative + 1):\n",
    "    prob += binom(n, k) * np.power(p_neg, k) * np.power(p_pos, n - k)\n",
    "    \n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**Question 7.** In [Topic 5, part 2](https://mlcourse.ai/articles/topic5-part2-rf/), section 2. \"Comparison with Decision Trees and Bagging\" we show how bagging and Random Forest improve classification accuracy as compared to a single decision tree. Which of the following is a better explanation of the visual difference between decision boundaries built by a single desicion tree and those built by ensemble models?\n",
    "\n",
    " 1. Ensembles ignore some of the features. Thus picking only important ones, they build a smoother decision boundary \n",
    " \n",
    " \n",
    " 2. **Some of the classification rules built by a decision tree can be applied only to a small number of training instances +**\n",
    " \n",
    " \n",
    " 3. When fitting a decision tree, if two potential splits are equally good in terms of information criterion, then a random split is chosen. This leads to some randomness in building a decision tree. Therefore its decision boundary is so jagged\n",
    " \n",
    " ___\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8.** Random Forest learns a coefficient for each input feature, which shows how much this feature influences the target feature. True/False?\n",
    "\n",
    "Не понял вопроса\n",
    "\n",
    " 1. True\n",
    " 1. **False +**\n",
    " ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9.** Suppose we fit `RandomForestRegressor` to predict age of a customer (a real task actually, good for targeting ads), and the maximal age seen in the dataset is 98 years. Is it possible that for some customer in future the model predicts his/her age to be 105 years?\n",
    "\n",
    "Лес не умеет предсказывать\n",
    "\n",
    " 1. Yes\n",
    " \n",
    " \n",
    " 2. **No +**\n",
    " \n",
    " ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10.** Select all statements supporting advantages of Random Forest over decision trees (some statements might be true but not about Random Forest's pros, don't select those).\n",
    "\n",
    " 1. Random Forest is easier to train in terms of computational resources\n",
    " \n",
    " \n",
    " 2. **Random Forest typically requires more RAM than a single decision tree  +**\n",
    " \n",
    " \n",
    " 3. Random Forest typically achieves better metrics in classification/regression tasks\n",
    " \n",
    " \n",
    " 4. **Single decision tree's prediction can be much easier interpreted +** из лекции"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
